{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOR/JNXH7PM/7o73tfzVcN3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shinnew9/llms_from_scratch/blob/main/llms_from_scracth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUaMXLJNYfwY",
        "outputId": "3c3ecdbb-5776-4359-d4e4-4710fe363d66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers) (0.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "!!pip install datasets\n",
        "!pip install tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Load dataset"
      ],
      "metadata": {
        "id": "MDpjEav6Y9pa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load the data and separate into train, validation and test data\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm_gui\n",
        "\n",
        "os.mkdir(\"./malaygpt\")\n",
        "os.mkdir(\"./tokenizer_en\")\n",
        "os.mkdir(\"./tokenizer_my\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-ms\", split='train')\n",
        "val_dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-ms\", split='validation')\n",
        "\n",
        "# limit the number of data in dataset for faster training purpose\n",
        "raw_train_dataset, rt_to_skip = random_split(train_dataset, [1500, len(train_dataset)-1500])\n",
        "raw_val_dataset, vt_to_skip = random_split(val_dataset, [50, len(val_dataset)-50])"
      ],
      "metadata": {
        "id": "4wSJTApvY4wK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Create Tokenizer"
      ],
      "metadata": {
        "id": "d8aISJQHaE9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Create tokenizers\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "def get_ds_iterator(raw_train_dataset, lang):\n",
        "  for data in raw_train_dataset:\n",
        "    yield data['translation'][lang]\n",
        "\n",
        "# Create Source Tokenizer - English\n",
        "tokenizer_en = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "trainer_en = BpeTrainer(min_frequency=2, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
        "# We'll also need to add a pre-tokenizer to split our input into words as without a pre-tokenizer, we might get tokens that overlap several words: for instance we could get a \"there is\" token since those two words often appear next to each other.\n",
        "# Using a pre-tokenizer will ensure no token is bigger than a word returned by the pre-tokenizer.\n",
        "tokenizer_en.pre_tokenizer = Whitespace()\n",
        "tokenizer_en.train_from_iterator(get_ds_iterator(raw_train_dataset, \"en\"), trainer = trainer_en)\n",
        "tokenizer_en.save(\"./tokenizer_en/tokenizer_en.json\")\n",
        "\n",
        "# Create Target Tokenizer - Malay\n",
        "tokenizer_my = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "trainer_my = BpeTrainer(min_frequency=2, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MNSK]\"])\n",
        "tokenizer_my.pre_tokenizer = Whitespace()\n",
        "tokenizer_my.train_from_iterator(get_ds_iterator(raw_train_dataset, \"ms\"), trainer=trainer_my)\n",
        "tokenizer_my.save(\"./tokenizer_my/tokenizer_my.json\")\n",
        "\n",
        "tokenizer_en = Tokenizer.from_file(\"./tokenizer_en/tokenizer_en.json\")\n",
        "tokenizer_my = Tokenizer.from_file(\"./tokenizer_my/tokenizer_my.json\")\n",
        "\n",
        "source_vocab_size = tokenizer_en.get_vocab_size()\n",
        "target_vocab_size = tokenizer_my.get_vocab_size()\n",
        "\n",
        "# to calculate the max sequence length in the entire training dataset for the source and target dataset\n",
        "max_seq_len_source = 0\n",
        "max_seq_len_target = 0\n",
        "\n",
        "for data in raw_train_dataset:\n",
        "  enc_ids = tokenizer_en.encode(data['translation']['en']).ids\n",
        "  dec_ids = tokenizer_en.encode(data['translation']['ms']).ids\n",
        "  max_seq_len_source = max(max_seq_len_source, len(enc_ids))\n",
        "  max_seq_len_target = max(max_seq_len_target, len(dec_ids))\n",
        "\n",
        "print(f'max_seqlen_source: {max_seq_len_source}')  # 99 - can be different in your case\n",
        "print(f'max_seqlen_target: {max_seq_len_target}')  # 100 - can be different in your case\n",
        "\n",
        "\n",
        "# to make it standard for our training we'll just take max_seq_len_source and add 20-5- to cover the additional tokens such as PAD, CLS, SEP\n",
        "max_seq_len = 155"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28qrTyeiY4nQ",
        "outputId": "614a6a37-f918-4708-894b-7fb712958bc0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_seqlen_source: 408\n",
            "max_seqlen_target: 680\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Prepare Dataset and DataLoader"
      ],
      "metadata": {
        "id": "_-wzaqjZgnpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Prepare dataset and dataloader\n",
        "\n",
        "# Transform raw dataset to the encoded dataset that can be processed by the model\n",
        "class EncodeDataset(Dataset):\n",
        "  def __init__(self, raw_dataset, max_seq_len):\n",
        "    super().__init__()\n",
        "    self.raw_dataset = raw_dataset\n",
        "    self.max_seq_len = max_seq_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.raw_dataset)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    # fetching the single data for the given index value that consist of both english and malay language.\n",
        "    raw_text = self.raw_dataset[index]\n",
        "\n",
        "    # separating source text with enligh tokenizer and target text with malay tokenizer\n",
        "    source_text = raw_text['translation']['en']\n",
        "    target_text = raw_text['translation']['ms']\n",
        "\n",
        "    # Encoding source text with enlish tokenizer and target text with malay tokenizer\n",
        "    source_text_encoded = tokenizer_en.encode(source_text).ids\n",
        "    target_text_encoded = tokenizer_en.encode(target_text).ids\n",
        "\n",
        "    # Convert the CLS, SEP and PAD tokens to their corresponding index id in vocabulary using tokenizer [the id would be same with either tokenizers]\n",
        "    CLS_ID = torch.tensor([tokenizer_my.token_to_id(\"[CLS]\")], dtype=torch.int64)\n",
        "    SEP_ID = torch.tensor([tokenizer_my.token_ti_id(\"[SEP]\")], dtype=torch.int64)\n",
        "    PAD_ID = torch.tensor([tokenizer_my.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
        "\n",
        "    # To train the model, the sequence length of each input should be equal max seq length. Hence additional number of padding will be added to the input sequence if the length is not equal to the max seq length.\n",
        "    num_source_padding = self.max_seq_len - len(source_text_encoded) - 2\n",
        "    num_target_padding = self.max_seq_len - len(source_text_encoded) - 1\n",
        "\n",
        "    encoder_padding = torch.tensor([PAD_ID] * num_source_padding, dtype = torch.int64)\n",
        "    decoder_padding = torch.tensor([PAD_ID] * num_target_padding, dtype = torch.int64)\n",
        "\n",
        "    # encoder_input has the first token as start of sentence - CLS_ID, followed by source encoding which is then followed by the end of sentence token - SEP.\n",
        "    # To reach the required max_seq_len, addition PAD token will be added at the end.\n",
        "    encoder_input = torch.cat([CLS_ID, torch.tensor(source_text_encoded, dtype=torch.int64), SEP_ID, encoder_padding], dim=0)\n",
        "\n",
        "    # decoder_input has the first token as start of sentence - CLS_ID, followed by target encoding.\n",
        "    # To reach the required max_seq_len, addition PAD token will be added at the end. There is no end of sentence token - SEP in decoder input.\n",
        "    decoder_input = torch.cat([CLS_ID, torch.tensor(target_text_encoded, dtype=torch.int64), decoder_padding], dim=0)\n",
        "\n",
        "    # target_label is required for the loss calculation during training to compare between the predidcted and target label.\n",
        "    # target_label has the first token as target encoding followed by actual target encoding. There is no staart of sentence token - CLS in target label.\n",
        "    # To reach the required max_seq_len, addition PAD tokens will be added at the end.\n",
        "    target_label = torch.cat([torch.tensor(target_text_encoded, dtype=torch.int64), SEP_ID, decoder_padding], dim=0)\n",
        "\n",
        "    # SInce we've added extra padding token with input encoding, we dpn't want this token to be trained by model.\n",
        "    # So, we'll use encoder mask to nullify the padding value prior to producing output of self attention in encoder block\n",
        "    encoder_mask = (encoder_input != PAD_ID). unsqueeze(0).int()\n",
        "\n",
        "    # We don't want any token to get influence the future token during the decoding stage. Hence, Causal mask is being implemented during masked multihead attention to handle this.\n",
        "    decoder_mask = (decoder_input != PAD_ID).unsqueeze(0).unsqueeze(0).int() & causal_mask(decoder_input. size(0))\n",
        "\n",
        "\n",
        "    return {\n",
        "        'encoder_input': encoder_input,\n",
        "        'decoder_input': decoder_input,\n",
        "        'target_label': target_label,\n",
        "        'encoder_mask': encoder_mask,\n",
        "        'decoder_mask': decoder_mask,\n",
        "        'source_text': source_text,\n",
        "        'target_text': target_text\n",
        "    }\n",
        "\n",
        "# Causal mask will make sure any token that comes after the current token will be masked meaning the value will be replaced by - infinity that will be converted to zero or nearly zero after softmax operation.\n",
        "# Hence the model will just ignore these vue or won't be able to learn anything.\n",
        "def causal_mask(size):\n",
        "  # Creating a square matrix of dimensions 'size x size' filled with ones\n",
        "  mask = torch.triu(torch.ones(1, size, size), diagonal = 1), type(torch.int)\n",
        "  return mask == 0\n",
        "\n",
        "# Create a dataloader to use for model training and validation\n",
        "train_ds = EncodeDataset(raw_train_dataset, max_seq_len)\n",
        "val_ds = EncodeDataset(raw_val_dataset, max_seq_len)\n",
        "\n",
        "train_dataloader = DataLoader(train_ds, batch_size = 5, shuffle=True)\n",
        "val_dataloader = DataLoader(val_ds, batch_size = 1, shuffle=True)"
      ],
      "metadata": {
        "id": "2WGYqr5GY4aG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Input Embedding and Positional Encoding"
      ],
      "metadata": {
        "id": "WnctbqxuuGBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Input embedding and positional encoding\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class EmbeddingLayer(nn.Module):\n",
        "  def __init__(self, d_model: int, vocab_size: int):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    # using pytorch models embedding layer to map token id to embedding vector which has the shpae of (vocab_size, d_model)\n",
        "    # The vocab_size is the vocabulary size of the training data created by tokenizer in step 2\n",
        "    self.embeding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "  def forward(self, input):\n",
        "    # In addition of giving input to the embedding, the extra multiplication by square root of d_model is to normalize the embedding layer output\n",
        "    embedding_output = self.embedding(input)*math.sqrt(self.d_model)\n",
        "    return embedding_output\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model:int, max_seq_len: int, dropout_rate:float):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "    pe = torch.zeros(max_seq_len, d_model)\n",
        "\n",
        "    pos = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * 0 (-math.log(10000.0 / d_model)))\n",
        "\n",
        "    pe[:, 0::2]= torch.sin(pos*div_term)\n",
        "    pe[:, 1::2]= torch.cos(pos*div_term)\n",
        "\n",
        "    # since we're expecting the input sentences in batches so the extra dimension to cater batch number needs to be added in 0 position\n",
        "    pe = pe.unsqueeze(0)\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, input_embedding):\n",
        "    input_embedding = input_embedding + (self.pe[:, :input_embedding.shape[1], :]).requires_grad(False)\n",
        "    return self.dropout(input_embedding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYXsLv2qphkf",
        "outputId": "b47cf0ed-89ff-41be-f91c-50ccb95e72cd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:26: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
            "<>:26: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
            "<ipython-input-7-5950faaa4dc4>:26: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
            "  div_term = torch.exp(torch.arange(0, d_model, 2).float() * 0 (-math.log(10000.0 / d_model)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Multi-Head Attention\n"
      ],
      "metadata": {
        "id": "dnaILWjJNIGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Multihead Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model: int, num_heads:int, dropout_rate:float):\n",
        "    super().__init__()\n",
        "    # Defining dropout to prevent overfitting\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "    self.num_heads = num_heads\n",
        "    assert d_model % num_heads == 0, \"d_model must be divisible by number of heads\"\n",
        "\n",
        "    # d_k is the new dimension of each self attention heads\n",
        "    self.d_k = d_model // num_heads\n",
        "\n",
        "    # Weight matrix are defined which are all learnable parameters\n",
        "    self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
        "    self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
        "    self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
        "    self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "  def forward(self, q, k, v, encoder_mask):\n",
        "    # Please note that we'll be training our model with not just a single sequence but rather batches of sequence, hence we'll include batch_size in the shape\n",
        "    # query, Key and value are calculated by matrix multiplication of corresponding weights with the input embeddings\n",
        "    # Change of shape: q(batch_size, seq_len, d_model) @ W_q(d_model, d_model) => query(batch_size, seq_len, d_model) [same goes to key and value]\n",
        "    query = self.W_q(q)\n",
        "    key = self.W_k(k)\n",
        "    value = self.W_v(v)\n",
        "\n",
        "    # Dividing query, key and value into number of heads, hence new dimension will be d_k.\n",
        "    # Change of shape: query(batch_size, seq_len, d_model) => query(batch_size, seq_len, num_heads, d_k) -> query (batch_size, num_heads, seq_len, d_k) [sam goes to key and value]\n",
        "    query = query.view(query.shape[0], query.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "    key = key.view(key.shape[0], key.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "    value = value.view(value.shape[0], value.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    # :: SELF ATTENTION BLOCK STARTS ::\n",
        "\n",
        "    # Attention score is calculated to find the similarity of rleation of query with key of itself and all other embedding in the sequence\n",
        "    # Change of shape: query(batch_size, num_heads, seq_len, d_k) @ key(batch_size,num_heqds, seq_len, d_k) => attention_score(batch_size, num_heads, seq_len, seq_len)\n",
        "    attention_score = (query @ key.transpose(-2, -1))/math.sqrt(self.d_k)\n",
        "\n",
        "    # If mask is provided the attention score needs to modify as per the mask value. Refer to the details in point no 4.\n",
        "    if encoder_mask is not None:\n",
        "      attention_score.masked_fill_(encoder_mask==0, -1e9)\n",
        "\n",
        "    # Softmax operation calculates the probability distribution among all the attention scores. This will determine which embedding is more similar to the griven query embedding and assign the attention weight accordingly.\n",
        "    # Change of shape: same as attention_score\n",
        "    attention_score = attention_score.softmax(dim=-1)\n",
        "\n",
        "    if self.dropout is not None:\n",
        "      attention_score = self.dropout(attention_score)\n",
        "\n",
        "    # Final step of Self attetion block is to marix multiplication of attention_weight with value embedding.\n",
        "    # Change of shape: attention_score (batch_size, num_heqds, seq_len, seq_len) @ value(batch_size, num_heads, seq_len, num_heads, d_k)\n",
        "    attention_output = attention_score @ value\n",
        "\n",
        "    # :: SELF ATTENTION BLOCK ENDS ::\n",
        "\n",
        "    # Now, all the heads will be concated back to for a single head\n",
        "    # Change of shape:attention_output(batch_size, num_heads, seq_len, d_k) => attention_output(batch_size, seq_len, num_heads, d_k) => attention_output(batch_size, seq_len, d_model)\n",
        "    attention_output = attention_output.transpose(1, 2).contiguous().view(attention_output.shape[0], -1, self.num_heads * self.d_k)\n",
        "\n",
        "    # Finally attention_output is matrix multiplied with output weight matrix to give the final Multi-Head Attention output.\n",
        "    # The shape of the multihead_output is same as the embedding input\n",
        "    # Change of shape: attentipn_output(bath_size, seq_len, d_model) @ W_o(d_model, d_model) => multihead_output(batch_size, seq_len, d_model)\n",
        "    multihead_output = self.w_o(attention_output)\n",
        "\n",
        "    return multihead_output"
      ],
      "metadata": {
        "id": "lLh_ttspNPnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FD9zK3wJTn5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vPVS2NCRS9_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "m7OXbnbYS97n"
      }
    }
  ]
}