{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOByCBmhr0w5aGkJbdKU0L2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shinnew9/llms_from_scratch/blob/main/Llam3_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/towards-artificial-intelligence/build-your-own-llama-3-architecture-from-scratch-using-pytorch-2ce1ecaa901c"
      ],
      "metadata": {
        "id": "8rLO9hiZIqFG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbUTysH922HD"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple, List\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "### Step 1: Input Block ###\n",
        "# Using Tiny Shakespeare dataset for character-level tokenizer. Some part of the following character-level tokenizer is referenced from Andrej karpathy's GitHub( https://github.com/karpathy/nanoGPT/blob/master/data/shakespeare_char/prepare.py) which I found is explained very well.\n",
        "# Load tiny_shakespeare data file (https://girhub.com/tamangmilan/llama3/blob/main/tiny_shakespeare.txt)\n",
        "\n",
        "device:str = 'cuda' if torch.cuda.is_available() else 'cpu'   # Assign device to cuda or cpu based on availability\n",
        "\n",
        "# Load tiny_shakespeare data file.\n",
        "with open('tiny_shakespeare.txt', 'r') as f:\n",
        "  data = f.read()\n",
        "\n",
        "# Prepare vocabulary by taking all the unique characters from tiny_shakespeare data\n",
        "vocab = sorted(list(set(data)))\n",
        "\n",
        "# Training Llama3 model requires additional tokens such as <|begin_of_text|>, <|end_of_text|> and <|pad_id|>, we'll add them into vocabulary\n",
        "# This is importatnt to build tokenizers encdoe and decode functions.\n",
        "itos = {i:ch for i, ch in enumerate(vocab)}\n",
        "stoi = {ch:i for i, ch in enumerate(vocab)}\n",
        "\n",
        "# Tokenizers encode function: take a string, output a list of integers\n",
        "def encode(s):\n",
        "  return [stoi[ch] for ch in s]\n",
        "\n",
        "# Tokenizers decode function: take a list of integers, output a string\n",
        "def decode(l):\n",
        "  return ''.join(itos[i] for i in l)\n",
        "\n",
        "# Define tensor token variable to be used later during model training\n",
        "token_bos = torch.tensor([stoi['<|being_of_text>']], dtype=torch.int, device=device)\n",
        "token_eos = torch.tensor([stoi['<|end_of_text>']], dtype=torch.int, device=device)\n",
        "token_pad = torch.tensor([stoi['<|pad_id>']], dtype=torch.int, device=device)\n",
        "\n",
        "prompts = \"Hello World\"\n",
        "encoded_tokens = encode(prompts)\n",
        "decoded_text = decode(encoded_tokens)\n",
        "\n",
        "### Test: Input Block Code ###\n",
        "# You need take out the triple quotes below to perform testing\n",
        "\"\"\"\n",
        "print(f\"Length of shakespeare in character: {len(data)}\")\n",
        "print(f\"The vocabulary looks like this: {''.join(vocab)\\n\")\n",
        "print(f\"Vocab size: {vocab_size}\")\n",
        "print(f\"encoded_tokens: {encoded_tokens}\")\n",
        "print(f\"decoded_text: {decoded_text}\")\n",
        "\"\"\"\n",
        "### Test Results; ###\n",
        "\"\"\"\n",
        "Length of shakespeare in characater: 1115394\n",
        "The vocabulary looks like this:\n",
        "  !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz<|begin_of_text|><|end_of_text|><|pad_id|>\n",
        "\n",
        "Vocab size: 68\n",
        "encoded_tokens: [20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42]\n",
        "decoded_text: Hello World\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: The Decoder Block\n",
        "# Note: Since the Llama 3 model is developed by Meta, so to be in sync with their codebase and for future compatibility,\n",
        "# I will use most of the code from Meta GitHub with some necessary changes required to achieve our goal.\n"
      ],
      "metadata": {
        "id": "RowNZDeSEWiu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}