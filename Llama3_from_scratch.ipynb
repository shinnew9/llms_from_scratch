{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNjh8z2VCYQjsdTwoDcoMqY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shinnew9/llms_from_scratch/blob/main/Llama3_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/towards-artificial-intelligence/build-your-own-llama-3-architecture-from-scratch-using-pytorch-2ce1ecaa901c"
      ],
      "metadata": {
        "id": "8rLO9hiZIqFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import necessary libraries"
      ],
      "metadata": {
        "id": "qHAsHYaYPkCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJ4bxUgqCdFv",
        "outputId": "cffb7433-8c42-40b6-dab2-7588d3f2c326"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple, List\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# file_path = os.listdir(\"/content/drive/MyDrive/AI\")"
      ],
      "metadata": {
        "id": "aU53-bNgPgz7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcfONytED16I",
        "outputId": "9749c346-63fc-4f54-c15a-1a0d5f8e43ba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Input Block"
      ],
      "metadata": {
        "id": "Tojlkf-wPopZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qbUTysH922HD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "fbe4a94d-140f-475d-a2b7-ecf9ebfe910c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nLength of shakespeare in character: 1115394\\nThe vocabulary looks like this:\\n!$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz<|begin_of_text|><|end_of_text|><|pad_id|>\\n\\nVocab Size: 68\\nencoded_tokens: [20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42]\\ndecoded_text: Hello World\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Step 1: Input Block\n",
        "# Using Tiny Shakespeare dataset for character-level tokenizer. Some part of the following character-level tokenizer is referenced from Andrej karpathy's GitHub( https://github.com/karpathy/nanoGPT/blob/master/data/shakespeare_char/prepare.py) which I found is explained very well.\n",
        "# Load tiny_shakespeare data file (https://github.com/tamangmilan/llama3/blob/main/tiny_shakespeare.txt)\n",
        "\n",
        "device:str = 'cuda' if torch.cuda.is_available() else 'cpu'   # Assign device to cuda or cpu based on availability\n",
        "\n",
        "# Load tiny_shakespeare data file.\n",
        "with open('/content/drive/MyDrive/AI/tiny_shakespeare.txt', 'r') as f:\n",
        "  data = f.read()\n",
        "\n",
        "# Prepare vocabulary by taking all the unique characters from tiny_shakespeare data\n",
        "vocab = sorted(list(set(data)))\n",
        "\n",
        "# Training Llama3 model requires additional tokens such as <|begin_of_text|>, <|end_of_text|> and <|pad_id|>, we'll add them into vocabulary\n",
        "vocab.extend(['<|begin_of_text|>', '<|end_of_text|>', '<|pad_id|>'])\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Create a mapping between characters with corresponding integer indexes in vocabulary.\n",
        "# This is important to build tokenizers encode and decode functions.\n",
        "itos = {i:ch for i, ch in enumerate(vocab)}\n",
        "stoi = {ch: i for i, ch in enumerate(vocab)}\n",
        "\n",
        "# Tokenizers encode function: take a string, output a list of integers\n",
        "def encode(s):\n",
        "  return [stoi[ch] for ch in s]\n",
        "\n",
        "# Tokenizers decode function: take a list of integers, output a string\n",
        "def decode(l):\n",
        "  return ''.join(itos[i] for i in l)\n",
        "\n",
        "# Define tensor token variable to be used later during model training\n",
        "token_bos = torch.tensor([stoi['<|begin_of_text|>']], dtype=torch.int, device=device)\n",
        "token_eos = torch.tensor([stoi['<|end_of_text|>']], dtype=torch.int, device=device)\n",
        "token_pad = torch.tensor([stoi['<|pad_id|>']], dtype=torch.int, device=device)\n",
        "\n",
        "prompts = \"Hello World\"\n",
        "encoded_tokens = encode(prompts)\n",
        "decoded_tokens = decode(encoded_tokens)\n",
        "\n",
        "### Test: Input Block Code ###\n",
        "# You need to take out the tirple quotes below to perform testing\n",
        "\"\"\"\n",
        "print(f\"Length of shakespeare in character: {len(data)}\")\n",
        "print(f\"The vocabulary looks like this: {''.join(vocab)}\\n\")\n",
        "print(f\"Vocab Size: {vocab_size}\")\n",
        "print(f\"encoded_tokens: {encoded_tokens}\")\n",
        "print(f\"decoded_tokens: {decoded_tokens}\")\n",
        "\"\"\"\n",
        "\n",
        "### Test Results: ###\n",
        "\"\"\"\n",
        "Length of shakespeare in character: 1115394\n",
        "The vocabulary looks like this:\n",
        "!$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz<|begin_of_text|><|end_of_text|><|pad_id|>\n",
        "\n",
        "Vocab Size: 68\n",
        "encoded_tokens: [20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42]\n",
        "decoded_text: Hello World\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: The Decoder Block"
      ],
      "metadata": {
        "id": "nj13zeVmrLXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: The Decoder Block\n",
        "# Note: Since the Llama 3 model is developed by Meta, so to be in sync with their codebase and for future compatibility,\n",
        "# I will use most of the code from Meta GitHub with some necessary changes required to achieve our goal.\n",
        "\n",
        "\n",
        "# Define parameters dataclass: we'll use these parameters during model building, training and inference.\n",
        "# Note: Since we want to see the results of training and inferencing faster rather than focusing on high accuracy, we're taking lower values for most of parameters which are set higher in the Llama 3 model.\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelArgs:\n",
        "  dim: int= 512           # embedding dimension\n",
        "  n_layers: int = 8       # number of model decoder blocks\n",
        "  n_heads: int = 8        # number of heads for queries embedding\n",
        "  n_kv_heads: int = 4     # number of heads for keys and values embedding\n",
        "  vocab_size: int = len(vocab)  # Length of vocabulary\n",
        "  multiple_of: int = 256        # Require to calculate dim of feedforward network\n",
        "  ffn_dim_multiplier: Optional[float] = None  # Require to calculate dime of feedforward network\n",
        "  norm_eps: float = 1e-5        # Default Epsilon value set for the RMSNorm calculation\n",
        "  rope_theta: float = 10000.0   # Default theta value for the RePE calculation\n",
        "\n",
        "  max_batch_size: int = 10      # Max batch size\n",
        "  max_seq_len: int = 256        # Max sequence length\n",
        "\n",
        "  epochs: int = 2500            # Total number of training iteration\n",
        "  log_interval: int = 10        # Number of interval to print to logs and loss values\n",
        "  device: str = 'cuda' if torch.cuda.is_available() else 'cpu'   # Assign device to cuda or cpu based on availabtility"
      ],
      "metadata": {
        "id": "RowNZDeSEWiu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2a: The RMSNorm\n",
        "class RMSNorm(nn.Module):\n",
        "  def __init__(self, dim: int, eps: float= 1e-6):\n",
        "    super().__init__()\n",
        "    device = ModelArgs.device\n",
        "    self.eps = eps\n",
        "    # Scaling parameter gamma, initialized with one and the no of parameters is euql to the size of dim\n",
        "    self.weight = nn.Parameter(torch.ones(dim).to(device))\n",
        "\n",
        "  def _norm(self, x):\n",
        "    return x*torch.rsqrt(x.pow(2).mean(dim=-1, keepdim=True)+ self.eps).to(device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Shape: x[bs, seq, dim]\n",
        "    output = self._norm(x.float()).type_as(x)\n",
        "\n",
        "    # Shape: x[bs, seq, dim] -> x_norm[bs, seq, dim]\n",
        "    return output*self.weight\n",
        "\n",
        "### Test: RMSNorm Code ###\n",
        "# You need take out the triple quotes below to perform testing\n",
        "\"\"\"\n",
        "x = torch.randn((ModelArgs.max_batch_size, ModelArgs.max_seq_len, ModelArgs.dim))\n",
        "rms_norm = RMSNorm(dim=ModelArgs.dim)\n",
        "x_norm = rms_norm(x)\n",
        "\n",
        "print(f\"Shape of x: {x.shape}\")\n",
        "print(f\"Shape of x_norm: {x_norm.shape}\")\n",
        "\"\"\"\n",
        "\n",
        "### Test Results: ###\n",
        "\"\"\"\n",
        "Shape of x: torch.Size([10, 256, 512])\n",
        "Shape of x_norm: torch.Size([10, 256, 512])\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9lRxwvkAIB-S",
        "outputId": "cd6a27ee-72ab-44c7-e276-b41a5e253975"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nShape of x: torch.Size([10, 256, 512])\\nShape of x_norm: torch.Size([10, 256, 512])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2b: The RoPE\n",
        "def precompute_freqs_cis(dim:int, seq_len:int, theta: float=10000.0):\n",
        "  # Computing Theta value for each dim pair which is dim/2\n",
        "  device = ModelArgs.device\n",
        "  freqs = 1.0/(theta**(torch.arange(0, dim, 2, device=device)[:(dim//2)].float()/dim))\n",
        "\n",
        "  # Computing range of positions(m) in the sequence\n",
        "  t = torch.arange(seq_len, dtype=torch.float32, device=device)\n",
        "\n",
        "  # freqs gives all the Theta value range for all the position of tokens in the sequence\n",
        "  freqs = torch.outer(t, freqs).to(device)\n",
        "\n",
        "  # This is the rotation matrix which needs to be converted to Polar from in order\n",
        "  freq_cis = torch.polar(torch.ones_like(freqs).to(device), freqs).to(device)\n",
        "  return freq_cis\n",
        "\n",
        "\n",
        "def reshape_for_broadcast(freqs_cis, x):\n",
        "  ndim = x.ndim\n",
        "  assert 0 <=1 <ndim\n",
        "  assert freqs_cis.shape == (x.shape[1], x.shape[-1]), \"the last two dimension of \"\n",
        "  shape = [d if i==1 or i==ndim-1 else i for i, d in enumerate(x.shape)]\n",
        "  return freqs_cis.view(*shape)\n",
        "\n",
        "\n",
        "def apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor):\n",
        "  device = ModelArgs.device\n",
        "  # Applying rotary positional encoding to both query and key embedding together\n",
        "  # First: The last dimension of xq and xk embedding needs to be reshaped to make it a pair. As rotation matrix is applied to each pair of dim.\n",
        "  # Next: convert both xq and xk to complex number as the rotation matrix is only applicable to complex number\n",
        "  xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2)).to(device)  # xq_: [bsz, seq_len, n_heads, head_dim/2]\n",
        "  xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2)).to(device)  # xk_: [bsz, seq_len, n_heads, head_dim/2]\n",
        "\n",
        "  # The rotation matrix(freqs_cis) dimensions across seq_len(dim=1) and head_dim(dim=3) should match with the embedding\n",
        "  # Also, the shape freqs_cis should be the same with xq and xk, hence change the shape of freqs_cis: [seq_len, head_dim] _> freqs_cis[:1, seq_len, 1, head_dim]\n",
        "  freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
        "\n",
        "  # Finally, perform rotation operation by multiplying iwth freqs_cis.\n",
        "  # After the rotation is completed, convert both xq_out and xk_out back to real number and return\n",
        "  xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3).to(device)  # xq_out: [bsz, seq_len, n_heads, head_dim]\n",
        "  xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3).to(device)  # xk_out: [bsz, seq_len, n_heqads, head_dim]\n",
        "  return xq_out.type_as(xq), xk_out.type_as(xk)\n",
        "\n",
        "\n",
        "### Test: RoPE Code ###\n",
        "# Note: x_norm is calculated during RMSNorm and is being used for testing here.\n",
        "# You need take out the triple quotes below to perform testing\n",
        "\"\"\"\n",
        "head_dim = ModelArgs.dim/ModelArgs.n_heads\n",
        "wq = nn.Linear(ModelArgs.dim, ModelArgs.n_heads * head_dim, bias=False, device=device)\n",
        "wk = nn.Linear(ModelArgs.dim, ModelArgs.n_kv_heads * head_dim, bias=False, device=device)\n",
        "xq = wq(x_norm)\n",
        "xk = wk(x_norm)\n",
        "print(f\"xq.shape: {xq.shape}\")\n",
        "print(f\"xk.shape: {xk.shape}\")\n",
        "\n",
        "xq = xq.view(xq.shape[0], xq.shape[1], ModelArgs.n_heads, head_dim)\n",
        "xq = xk.view(xk.shape[0], xk.shape[1], ModelArgs.n_kv_heads, head_dim)\n",
        "print(f\"xq.re-shape: {xq.shape}\")\n",
        "print(f\"xk.re-shape: {xk.shape}\")\n",
        "\n",
        "freqs_cis = precompute_freqs_cis(dim=head_dim, seq_len = ModelArgs.max_seq_len)\n",
        "print(f\"freqs_cis.shape: {freqs_cis.shape}\")\n",
        "\n",
        "xq_rotate, xk_rotate = apply_rotary_emb(xq, xk, freqs_cis)\n",
        "print(f\"xq_rotate.shape: {xq_rotate.shape}\")\n",
        "print(f\"xk_rotate.shape: {xk_rotate.shape}\")\n",
        "\"\"\"\n",
        "### Test Results: ###\n",
        "\"\"\"\n",
        "xq.shape: torch.Size([10, 256, 512])\n",
        "xk.shape: torch.Size([10, 256, 256])\n",
        "xq.re-shape: torch.Size([10, 256, 8, 64])\n",
        "xk.re-shape: torch.Size([10, 256, 4, 64])\n",
        "freqs_cis.shape: torch.Size([256, 32])\n",
        "xq_rotate.shape: torch.Size([10, 256, 8, 64])\n",
        "xk_rotate.shape: torch.Size([10, 256, 4, 64])\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "YW2iK5jzL68_",
        "outputId": "c5ca77bf-a6af-4e14-9a81-f0e064de30b6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nxq.shape: torch.Size([10, 256, 512])\\nxk.shape: torch.Size([10, 256, 256])\\nxq.re-shape: torch.Size([10, 256, 8, 64])\\nxk.re-shape: torch.Size([10, 256, 4, 64])\\nfreqs_cis.shape: torch.Size([256, 32])\\nxq_rotate.shape: torch.Size([10, 256, 8, 64])\\nxk_rotate.shape: torch.Size([10, 256, 4, 64])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: The Attention Block [Step2c: The KV Cache, Group Query Attention]\n"
      ],
      "metadata": {
        "id": "TSkc7ZCHPemX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: The Attention Block [Step2c: The KV Cache, Group Query Attention]\n",
        "# As mentioned before, the naming convention follows original the meta's LLaMa3 Github.\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, args: ModelArgs):\n",
        "    super().__init__()\n",
        "    self.args = args\n",
        "    # Embedding dimension\n",
        "    self.dim = args.dim\n",
        "    # Number of heads assigned to Query\n",
        "    self.n_heads = args.n_heads\n",
        "    # Number of heads assigned to Key and Values. If \"None\", the number will be\n",
        "    self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
        "    # Dimension of each head relative to model dimension\n",
        "    self.head_dim = args.dim // args.n_heads\n",
        "    # Number of repetition in order to make time Key, Value heads to match Query heads number\n",
        "    self.n_rep = args.n_heads // args.n_kv_heads\n",
        "\n",
        "    # Weight initialize for Keys, Query, Values and Output. Notice that the out_feature value of weight\n",
        "    self.wq = nn.Linear(self.dim, self.n_heads * self.head_dim, bias=False, device=device)\n",
        "    self.wk = nn.Linear(self.dim, self.n_kv_heads*self.head_dim, bias=False, device=device)\n",
        "    self.wv = nn.Linear(self.dim, self.n_kv_heads * self.head_dim, bias=False, device=device)\n",
        "    self.wo = nn.Linear(self.n_heads * self.head_dim, self.dim, bias=False, device=device)\n",
        "\n",
        "    # Initialize caches to store Key, Values at start. (KV Cache Implementation)\n",
        "    self.cache_k = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim), device=args.device)\n",
        "    self.cache_v = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim), device=args.device)\n",
        "\n",
        "\n",
        "  def forward(self, x: torch.Tensor, start_pos, inference):\n",
        "    # Shape of the input embedding: [bsx, seq_len, dim]\n",
        "    bsz, seq_len, _ = x.shape\n",
        "    # Mask will be used during 'Training' and is not required for 'inference' du\n",
        "    mask = None\n",
        "\n",
        "    xq = self.wq(x)  # x[bsz, seq_len, dim] * wq[dim, n_heads * heads_dim] -> q[bsz, seq_len]\n",
        "    xk = self.xk(x)  # x[bsz, seq_len, dim] * wq[dim, n_kv_heads * head_dim] -> k[bsz, seq_len, n_kv_heads * head_dim]\n",
        "    xv = self.wv(x)  # x[bsz, seq_len, dim] * wq[dim, n_kv_heads * head_dim] -> v[bsz, ]\n",
        "\n",
        "    # Model - Inference Mode: kv-cache is enabvled at inference mode only.\n",
        "    if inference:\n",
        "      # Compute rotation matrix for each position in the sequence\n",
        "      freq_cis = precompute_freqs_cis(dim = self.head_dim, seq_len = self.args.max_seq_len*2)\n",
        "      # During inferencing, we should only take the rotation matrix range from the current position of the\n",
        "      freqs_cis = freqs_cis[start_pos : start_pos + seq_len]\n",
        "      # xq[bsz, seq_len, n_heads, head_dim], xk[bsz, seq_len, n_heads, head_dim]\n",
        "      xq, xk = apply_rotary_emb(xq, xk, freqs_cis)\n",
        "\n",
        "      self.cache_k = self.cache_k.to(xq)\n",
        "      self.cache_v = self.cache_v.to(xq)\n",
        "\n",
        "      # Use repeat_kv function to make Keys, Values shape same as as the queries shape\n",
        "      # keys[bsz, seq_len, n_heads, head_dim], # Values[bsz, seq_len, n_heads, head_dim]\n",
        "      keys = repeat_kv(xk, self.n_rep)\n",
        "      values = repeat_kv(xv, self.n_rep)\n",
        "\n",
        "      # For training emb, mode, we'll compute mask and apply to the attention score\n",
        "      mask = torch.full((seq_len, seq_len), float(\"-inf\"), device=self.args.device)\n",
        "      mask = torch.triu(mask, diagonal=1).to(self.args.device)\n",
        "\n",
        "      # To compute attention, we'll need to perform a transpose operation to reshape\n",
        "      xq = xq.transpose(1, 2)       # xq[bsz, n_heads, seq_len, head_dim]\n",
        "      keys = keys.transpose(1, 2)   # keys[bsz, n_heads, seq_len, head_dim]\n",
        "      values = values.trasnpose(1, 2)   # values[bsz, n_heads, seq_len, head_dim]\n",
        "\n",
        "      # Computingattention score\n",
        "      scores = torch.matmul(xq, keys.transpose(2, 3)). to(self.args.device) / math.sqrt(self.head_dim)\n",
        "      if mask is not None:\n",
        "        scores = scores + mask\n",
        "\n",
        "      # Apply softmax to the attention score\n",
        "      scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
        "      # Matrix multiplication of attention score with the values\n",
        "      output = torch.matmul(scores, values).to(self.args.device)\n",
        "\n",
        "      # We get the contextual embedding for each head\n",
        "      # All heads need to be reshaped back and combined to give a single single\n",
        "      # Shape change: output[bsz, n_heads, seq_len, head_dim] -> output[bsz, seq_len, n_heads, head_dim] -> output[bsz, seq_len, n_heads * head_dim]\n",
        "      output = output.transpose(1, 2).contiguous().view(bsz, seq_len, -1)\n",
        "\n",
        "      # shape: output [bsz, seq_len, dim]\n",
        "      return self.wo(output)\n",
        "\n",
        "\n",
        "  # If the number of keys/values heads is less than query heads, this function\n",
        "  def repeat_kv(x:torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "    bsz, seq_len, n_kv_heads, head_dim = x.shape\n",
        "    if n_rep == 1:\n",
        "      return x\n",
        "    return (\n",
        "        x[:, :, :, None, :]\n",
        "        .expand(bsz, seq_len, n_kv_heads, n_rep, head_dim)\n",
        "        .reshape(bsz, seq_len, n_kv_heads * n_rep, head_dim)\n",
        "    )\n",
        "\n",
        "### Test: Repeat_kv function ###\n",
        "# note: xk, x_norm is already calculated during RoPE, RMSNorm testing and is being\n",
        "# You need take out the triple quotes belwo to perform testing\n",
        "\"\"\"\n",
        "n_rep = ModelArgs.n_heads // ModelArgs.n_kv_heads\n",
        "keys = repeat_kv(xk, n_rep)\n",
        "print(f\"xk.shape: {xk.shape}\")\n",
        "\n",
        "## Test: Attention function\n",
        "# You need take out the triple quotes below to perform testing\n",
        "\n",
        "attention = Attention(ModelArgs)\n",
        "x_out = attention(x_norm, start_pos= 0, inference=False)\n",
        "print(f\"x_out.shape: {x_out.shape}\")\n",
        "\"\"\"\n",
        "### Test Results: ###\n",
        "\"\"\"\n",
        "xk.shape: torch.Size([10, 256, 4, 64])\n",
        "keys.shape: torch.Size([10, 256, 8, 64])\n",
        "x_out.shape: torch.Size([10, 256, 512])\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "RvQL_fZ-VGW5",
        "outputId": "f5cf4738-ee37-4afc-ce6e-30688a9b7f1b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nxk.shape: torch.Size([10, 256, 4, 64])\\nkeys.shape: torch.Size([10, 256, 8, 64])\\nx_out.shape: torch.Size([10, 256, 512])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2e: The Feedforward Network (SwiGLU activation)"
      ],
      "metadata": {
        "id": "7MK_P5q1eJww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2e: The Feedforward Network (SwiGLU activation)\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, dim:int, hidden_dim: int, multiple_of:int, ffn_dim_multiplier: Optional[float]):\n",
        "    super().__init__()\n",
        "    # Models embedding dimension\n",
        "    self.dim = dim\n",
        "\n",
        "    # We must use the hidden dimensions calculation shared by Meta which is the ideal one for this model\n",
        "    # Hidden dimension are calculated such that it is a multiple of 256.\n",
        "    hidden_dim = int(2*hidden_dim/3)\n",
        "    if ffn_dim_multiplier is not None:\n",
        "      hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
        "    hidden_dim = multiple_of * ((hidden_dim + multiple_of -1 ) // multiple_of)\n",
        "\n",
        "    # define hidden layers weight\n",
        "    self.w1 = nn.Linear(self.dim, hidden_dim, bias=False, device=device)\n",
        "    self.w2 = nn.Linear(hidden_dim, self.dim, bias=False, device=device)\n",
        "    self.w3 = nn.Linear(self.dim, hidden_dim, bias=False, device=device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Shape: [bsz, seq_len, dim]\n",
        "    return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
        "\n",
        "  ### Test: FeedForward module ###\n",
        "  # note: x_out is already computed at Attention testing and is being used for testing here.\n",
        "  # You need to take out the triple quotes below to perform testing\n",
        "  \"\"\"\n",
        "  feed_forward = FeedForward(ModelArgs.dim, 4*ModelArgs.dim, ModelArgs.multiple_of, ModelArgs.ffn_dim_multiplier)\n",
        "  x_out = rms_norm(x_out)\n",
        "  x_out = feed_forward(x_out)\n",
        "  print(f\"feed forward output: x_out.shape: {x_out.shape}\")\n",
        "  \"\"\"\n",
        "\n",
        "  ### Test Results: ###\n",
        "  \"\"\"\n",
        "  feed forward output: x_out.shape: torcvh.Size([10, 256, 512])\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "hUkChYMqZjc8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2f: The Decoder Block."
      ],
      "metadata": {
        "id": "eCpw7yZTOTgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2f: The Decoder Block.\n",
        "# The class name is assigned as TransformerBlock\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, args: ModelArgs):\n",
        "    super().__init__()\n",
        "    self.args = args\n",
        "    # Initialize RMSNorm for attention\n",
        "    self.attention_norm = RMSNorm(dim=args.dim, eps=args.norm_eps)\n",
        "    # Initialize Attention class\n",
        "    self.attention = Attention(args)\n",
        "    # Initialize RMSNorm for feedforward class\n",
        "    self.ff_norm = RMSNorm(dim=args.dim, eps=args.norm_eps)\n",
        "    # Initialize feedforward class\n",
        "    self.feedforward = FeedForward(args.dim, 4*args.dim, args.multiple_of, args.ffn_dim_multiplier)\n",
        "\n",
        "  def forward(self, x, srat_pos, inference):\n",
        "    # start_pos = token position for inference mdoe, inference = True for inference\n",
        "    # i) pass input embedding to attention_norm and then pass to attention block\n",
        "    # ii) the output of attention is then added to embedding(before norm)\n",
        "    h = x + self.attention(self.attention_norm(x), start_pos, inference)\n",
        "\n",
        "    # i) pass attention output to ff_norm and then pass to the feedforward network\n",
        "    # ii) the output of feedforward network is then added to the attention output (before ff_norm)\n",
        "    out = h + self.feedforward(self.ff_norm(h))\n",
        "    # Shape: [bsz, seq_len, dim]\n",
        "    return out\n",
        "\n",
        "### Test: TransformerBlock ###\n",
        "# You need take out the triple quotes belwo to perform testing\n",
        "\"\"\"\n",
        "x = torch.randn(ModelArgs.max_batch_size, ModelArgs.max_seq_len, ModelArgs.dim)\n",
        "transformer_block = TransformerBlock(ModelArgs)\n",
        "transformer_block_out = transformer_block(x, start_pos = 0, inference = False)\n",
        "print(f\"transformer_block_out.shape: {transformer_block_out.shape}\")\n",
        "\"\"\"\n",
        "## Test Results: ###\n",
        "\"\"\"\n",
        "transformer_block_out.shape: torch.Size([10, 564, 128])\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "gudDxZkKYXMZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8fd46184-4080-4b1a-f0aa-4707956f2ed9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntransformer_block_out.shape: torch.Size([10, 564, 128])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step3: The Output Block"
      ],
      "metadata": {
        "id": "pMPHeeedYxy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 3: The Output Block\n",
        "# This is the Llama 3 model. Again, the class name is maintained as Transformer to match with Meta Llama 3 model.\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, params: ModelArgs):\n",
        "    super().__init__()\n",
        "    # set all the ModelArgs in params variable\n",
        "    self.params = params\n",
        "    # Initialize embedding class from the input block\n",
        "    self.tok_embeddings = nn.Embedding(params.vocab_size, params.dim)\n",
        "\n",
        "    # Initialize the decoder block and store it inside the ModuleList.\n",
        "    # This is because we've 4 decoder blocks in our Llama 3 model. (Official Llama 3 has 32 blocks)\n",
        "    self.layers = nn.ModuleList()\n",
        "    for layer_id in range(params.n_layers):\n",
        "      self.layers.append(TransformerBlock(args=params))\n",
        "\n",
        "    # Initializate RMSNorm for the output block\n",
        "    self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
        "\n",
        "    # Initializate linear layer at the output block.\n",
        "    self.output = nn.Linear(params.dim, params.vocab_size, bias=False)\n",
        "\n",
        "  def forward(self, x, start_pos, targets=None):\n",
        "    # start_pos = token position for inference mode, inference = True for inference and False for training mode\n",
        "    # x is the batch of token_ids generated from the texts or prompts using tokenizers.\n",
        "    # x[bsz, seq_len] -> h[bsz, seq_len, dim]\n",
        "    h = self.tok_embeddings(x)\n",
        "\n",
        "    # If the target is none, Inference mode is activated and set to \"True\" and \"False\" if Training mode is activated.\n",
        "    if targets is None:\n",
        "      inference = True\n",
        "    else:\n",
        "      inference = False\n",
        "\n",
        "    # The embeddings (h) will then pass though all the decoder blocks.\n",
        "    for layer in self.layers:\n",
        "      h = layer(h, start_pos, inference)\n",
        "\n",
        "    # The output from the final decoder block will feed into the RMSNorm\n",
        "    h = self.norm(h)\n",
        "\n",
        "    # After normalized, the embedding h will then feed into the Linear layer.\n",
        "    # The main task of the Linear layer is to generate logits that maps the embedding with the vocabulary size.\n",
        "    # h[bsz, seq_len, dim] -> logits[bsz, seq_len, vocab_size]\n",
        "    logits = self.output(h).float()\n",
        "    loss = None\n",
        "\n",
        "    # Inference mode is activated if the targets is not available\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    # Training mode is activated if the targets are available. And Loss will be calculated for further model training.\n",
        "    else:\n",
        "      loss = F.cross_entropy(logits.view(-1, self.params.vocab_size), targets.view(-1))\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "\n",
        "### Test: Transformer (Llama Model) ###\n",
        "# You need take out the triple quotes below to perform testing\n",
        "\"\"\"\n",
        "model = Transformer(ModelArgs).to(ModelArgs.device)\n",
        "print(model)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IsblN08PZK4Y",
        "outputId": "1cffeb20-810c-426f-bc56-8f97351653a1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nmodel = Transformer(ModelArgs).to(ModelArgs.device)\\nprint(model)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Train our Llama 3 model:"
      ],
      "metadata": {
        "id": "PTrI0UlRex5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Train our Llama 3 model:\n",
        "\n",
        "# Create a dataset by encoding the entire tiny_shakespeare data token_ids list using the tokenizer's encode function that we've built at the input block section\n",
        "dataset = torch.tensor(encode(data), dtype=torch.int).to(ModelArgs.device)\n",
        "print(f\"dataset-shape: {dataset.shape}\")\n",
        "\n",
        "# Define function to generate batches from the given dataset\n",
        "def get_dataset_batch(data, split, args:ModelArgs):\n",
        "  seq_len = args.max_seq_len\n",
        "  batch_size = args.max_batch_size\n",
        "  device = args.device\n",
        "\n",
        "  train = data[:int(0.8 * len(data))]\n",
        "  val = data[int(0.8 * len(data)): int(0.9 * len(data))]\n",
        "  test = data[int(0.9 * len(data)):]\n",
        "\n",
        "  batch_data = train\n",
        "  if split == \"val\":\n",
        "    batch_data = val\n",
        "\n",
        "  if split == \"test\":\n",
        "    batch_data = test\n",
        "\n",
        "  # Picking random starting points from the dataset to give random samples for logging and plotting\n",
        "  ix = torch.randint(0, len(batch_data) - seq_len -3, (batch_size,)).to(device)\n",
        "  x = torch.stack([torch.cat([token_bos, batch_data[i:i+seq_len-1]]) for i in ix]).long().to(device)\n",
        "  y = torch.stack([torch.cat([batch_data[i+1:i+seq_len], token_eos]) for i in ix]).long().to(device)\n",
        "\n",
        "  return x, y\n",
        "\n",
        "### Test: get_dataset function ###\n",
        "\"\"\"\n",
        "xs, ys = get_dataset_batch(dataset, split=\"train\", args=ModelArgs)\n",
        "print([(decoe(xs[i].tolist()), decode(ys[i].tolist())) for i in range(len(xs))])\n",
        "\"\"\"\n",
        "\n",
        "# Define a evaluate loss function to calculate and store training and validation\n",
        "@torch.no_grad()\n",
        "def evaluate_loss(model, args:ModelArgs):\n",
        "  out= {}\n",
        "  model.eval()\n",
        "\n",
        "  for split in [\"train\", \"val\"]:\n",
        "    losses = []\n",
        "    for _ in range(10):\n",
        "      xb, yb = get_dataset_batch(dataset, split, args)\n",
        "      _, loss = model(x=xb, targets=yb)\n",
        "      losses.append(loss.item())\n",
        "    out[split] = np.mean(losses)\n",
        "\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "\n",
        "# Define a training function to perform model training\n",
        "def train(model, optimizer, args:ModelArgs):\n",
        "  epochs = args.epochs\n",
        "  log_interval = args.log_interval\n",
        "  device = args.device\n",
        "  losses = []\n",
        "  start_time = time.time()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    xs, ys = get_dataset_batch(dataset, 'train', args)\n",
        "    xs = xs.to(device)\n",
        "    ys = ys.to(device)\n",
        "    logits, loss = model(x=xs, targets=ys)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % log_interval == 0:\n",
        "      batch_time = time.time() - start_time\n",
        "      x = evaluate_loss(model, args)\n",
        "      losses += [x]\n",
        "      print(f\"Epoch {epoch} | val loss {x['val']:.3f} | Time{batch_time: .3f}\")\n",
        "      start_time = time.time()\n",
        "\n",
        "  # Print the final validation loss\n",
        "  print(\"validation loss: \", losses[-1]['val'])\n",
        "  # Display the interval losses in plot\n",
        "  return pd.DataFrame(losses).plot()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbfAPt3-exlg",
        "outputId": "9635bc9e-4022-4ba6-ceb4-22e8b79bf36d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset-shape: torch.Size([1115394])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Start training our Llama 3 model"
      ],
      "metadata": {
        "id": "171QzixX1dll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(ModelArgs).to(ModelArgs.device)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "train(model, optimizer, ModelArgs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "xZ2U17WiN7xE",
        "outputId": "af554f90-08fe-429b-afb3-b179e4420f27"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Transformer.forward() missing 1 required positional argument: 'start_pos'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-2a35923548ab>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelArgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-2e4214326162>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, args)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Transformer.forward() missing 1 required positional argument: 'start_pos'"
          ]
        }
      ]
    }
  ]
}